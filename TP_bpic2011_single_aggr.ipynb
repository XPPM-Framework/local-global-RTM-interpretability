{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TP_bpic2011_single_aggr.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-Su6ZxFplfv4"},"source":["# Benchmark Evaluation for Predictive Monitoring of Remaining Cycle Time of Business Processes"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575080676487,"user_tz":-600,"elapsed":4367,"user":{"displayName":"Catarina Moreira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyK0wF4TYO25RO2JRdFN_0gu6FArdL0ZYCutkO=s64","userId":"07726683220655856721"}},"id":"zYbKPD5pllxJ","outputId":"663dff6c-f662-4e8c-8cb9-9957e062df5e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# connect to local working directory on Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575080683092,"user_tz":-600,"elapsed":10962,"user":{"displayName":"Catarina Moreira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyK0wF4TYO25RO2JRdFN_0gu6FArdL0ZYCutkO=s64","userId":"07726683220655856721"}},"id":"R8xZ-jpYA1Pu","outputId":"ec46f435-3fdb-4f5d-b6f5-d7fe3973d369","colab":{"base_uri":"https://localhost:8080/","height":513}},"source":["# install required libraries\n","!pip install lime"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting lime\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/4be533df5151fcb48942515e95e88281ec439396c48d67d3ae41f27586f0/lime-0.1.1.36.tar.gz (275kB)\n","\r\u001b[K     |█▏                              | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.17.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.3.2)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.21.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.1.1)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (0.14.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.6.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.5)\n","Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (4.3.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->lime) (41.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.12.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image>=0.12->lime) (0.46)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.1)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.1.1.36-cp36-none-any.whl size=284191 sha256=c03ecc90aac71d21361112b4a9d805284e56da86ee5ba636eb0a4ed071553f68\n","  Stored in directory: /root/.cache/pip/wheels/a9/2f/25/4b2127822af5761dab9a27be52e175105772aebbcbc484fb95\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.1.1.36\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PDltAWzbwtJH","colab":{}},"source":["# Define main workspace directory path\n","MY_WORKSPACE_DIR = \"/content/drive/My Drive/Colab Notebooks/CAiSE/\"\n","#MY_WORKSPACE_DIR = \"/Users/catarina/Google Drive/Colab Notebooks/CAiSE/\"\n","\n","# add my working directory to the colab path\n","import sys\n","from sys import argv\n","\n","sys.path.append(MY_WORKSPACE_DIR)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575080683094,"user_tz":-600,"elapsed":10952,"user":{"displayName":"Catarina Moreira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyK0wF4TYO25RO2JRdFN_0gu6FArdL0ZYCutkO=s64","userId":"07726683220655856721"}},"id":"KM9pL7FxTRE7","outputId":"af4a6d90-0ab6-48b2-e2f4-2c74418db133","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Install tensorflow\n","try:\n","    # tensorflow_version only exists in Colab\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":5,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575080689455,"user_tz":-600,"elapsed":17304,"user":{"displayName":"Catarina Moreira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyK0wF4TYO25RO2JRdFN_0gu6FArdL0ZYCutkO=s64","userId":"07726683220655856721"}},"id":"9PPOPZm2VK8v","outputId":"c7171b71-d9a7-45a1-b881-6e8911a3a0ce","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","# keras / deep learning libraries\n","from tensorflow import keras\n","\n","# models\n","from tensorflow.keras.models import Sequential, Model, load_model, model_from_json\n","\n","# layers\n","from tensorflow.keras.layers import Dense, LSTM,InputLayer, Bidirectional, GRU, SimpleRNN\n","from tensorflow.keras.layers import Input, Lambda, Flatten, Dropout, AveragePooling1D \n","from tensorflow.keras.layers import AveragePooling2D, AveragePooling3D, MaxPooling1D\n","from tensorflow.keras.layers import MaxPooling2D, MaxPooling3D, BatchNormalization\n","\n","# optimizers\n","from tensorflow.compat.v1.train import Optimizer\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Nadam\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import SGD\n","\n","# callbacks\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","import xgboost as xgb\n","\n","# other keras functions\n","from tensorflow.keras.utils import Sequence, plot_model, to_categorical\n","from tensorflow.keras import metrics\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.initializers import glorot_uniform\n","\n","# sklearn library\n","import sklearn\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import GridSearchCV \n","from sklearn.cluster import KMeans\n","from sklearn.base import TransformerMixin\n","from sklearn.model_selection import KFold\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","\n","# LIME - Explainability\n","import lime\n","import lime.lime_tabular\n","from lime import submodular_pick; # not using this but useful later.\n","\n","# serialise models\n","from numpy import loadtxt\n","import pickle\n","\n","# visualization\n","#from misc.misc import *\n","from IPython.core.display import display, HTML\n","display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n","\n","%matplotlib inline\n","from matplotlib.pyplot import figure\n","import matplotlib.pyplot as plt\n","\n","import matplotlib.image as mpimg\n","import pylab as pl\n","from pylab import savefig\n","\n","import seaborn as sns\n","sns.set()\n","\n","from time import time\n","\n","import itertools\n","import pickle\n","import os\n","import pandas as pd\n","import numpy as np\n","import csv\n","from numpy import array\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<style>.container { width:90% !important; }</style>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iS_lAaxV_kM7"},"source":["## Predictive Process Monitoring: Remaining Time Prediction\n","\n","Given an event log of complete cases of a business process, and a prefix case of the process as obtained from an event stream, we want to predict a performance measure of such prefix case in the future. For example, we may wsnt to predict the time of this case until completion (or remaining time) or its outcomeat completion. A **prediction point** is a point in the future where the performance measure has a predicted value. A prediction is thus based o nthe predictor's knowledge of the history of the process until the prediction point as well as knowledge of the future until the predicted point. The former is warrented by the predictor's **memory** while the latter is based on  the predictor's **forecast**, i.e., predicting the future based based on the trend nd seasonal patern analysis. Finally, the prediction is performed based on  a**prediction** algorithm.\n","Since in real-life business processes the amount of uncertainty increases over time, the prediction task becomes more difficult and genersally less acurate. As such, predictions are made up to a specific point of time in the future, i.e., the time zone **h**. The choice of **h** depends on how fast the process evolves and on the prediction goal."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HWQwncapAgBk"},"source":["### Research Questions\n","\n","Research questions analysed in paper\n",":\n","  - What methods exist for predictive monitoring of remaining time of business processes?\n","  - How to classify methods for predictive monitoring of remaining time?\n","  - What tyoe of data has been used to evaluate these methods, and from which application domains?\n","  - What is the relative performance of these methods?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5j15wytLKmIS"},"source":["### Methodology\n","\n","<img href=\"https://www.dropbox.com/s/4uj3yll961chfau/predictive_process_monitoring_workflow.png\" />\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_d2GqHvvZVhZ"},"source":["#### Prefix Bucketing\n","\n","Two possible approaches used in machine-learning-based predictive process monitoring:\n","\n","  1. train a single predictor on the whole event log;\n","  2. employ a multiple predictor apporach by dividing the prefix traces in the historical log into several **buckets** and fitting a separate predictor for each bucket. \n","\n","The four most used bucketing methods in the literature are:\n","  1. Zero Bucketing\n","  2. Prefix length bucketing\n","  3. Cluster bucketing\n","  4. State Bucketing\n","\n","  \n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k0gPQqwGBHeu","colab":{}},"source":["def get_encoder(method, case_id_col=None, static_cat_cols=None, static_num_cols=None, dynamic_cat_cols=None, dynamic_num_cols=None, fillna=True, max_events=None):\n","\n","    if method == \"static\":\n","        return StaticTransformer(case_id_col=case_id_col, cat_cols=static_cat_cols, num_cols=static_num_cols, fillna=fillna)\n","\n","    elif method == \"last\":\n","        return LastStateTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, fillna=fillna)\n","\n","    elif method == \"agg\":\n","        return AggregateTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, boolean=False, fillna=fillna)\n","\n","    elif method == \"bool\":\n","        return AggregateTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, boolean=True, fillna=fillna)\n","    \n","    elif method == \"index\":\n","        return IndexBasedTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, max_events=max_events, fillna=fillna)\n","\n","    else:\n","        print(\"Invalid encoder type\")\n","        return None\n","\n","     \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UE3Kbcrfcg55","colab":{}},"source":["def get_bucketer(method, encoding_method=None, case_id_col=None, cat_cols=None, num_cols=None, n_clusters=None, random_state=None, n_neighbors=None):\n","\n","    if method == \"cluster\":\n","        bucket_encoder = get_encoder(method=encoding_method, case_id_col=case_id_col, dynamic_cat_cols=cat_cols, dynamic_num_cols=num_cols)\n","        clustering = KMeans(n_clusters, random_state=random_state)\n","        return ClusterBasedBucketer(encoder=bucket_encoder, clustering=clustering)\n","        \n","    elif method == \"state\":\n","        bucket_encoder = get_encoder(method=encoding_method, case_id_col=case_id_col, dynamic_cat_cols=cat_cols, dynamic_num_cols=num_cols)\n","        return StateBasedBucketer(encoder=bucket_encoder)\n","            \n","    elif method == \"single\":\n","        return NoBucketer(case_id_col=case_id_col)\n","\n","    elif method == \"prefix\":\n","        return PrefixLengthBucketer(case_id_col=case_id_col)\n","    \n","    elif method == \"knn\":\n","        bucket_encoder = get_encoder(method=encoding_method, case_id_col=case_id_col, dynamic_cat_cols=cat_cols, dynamic_num_cols=num_cols)\n","        return KNNBucketer(encoder=bucket_encoder, n_neighbors=n_neighbors)\n","\n","    else:\n","        print(\"Invalid bucketer type\")\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ifyq27ULcsf0"},"source":["##### Zero bucketing \n","\n","All prefix traces are considered to be ub the same bucket. As such, a single predictor is fit for all prefies in the prefix log."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PNgYX8KNVMO3","colab":{}},"source":["# All prefix traces are considered to be in the same bucket. As such, a single \n","# predictor is fit for all prefies in the prefix log.\n","class NoBucketer(object):\n","    \n","    def __init__(self, case_id_col):\n","        self.n_states = 1\n","        self.case_id_col = case_id_col\n","        \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def predict(self, X, y=None):\n","        return np.ones(len(X[self.case_id_col].unique()), dtype=np.int)\n","    \n","    def fit_predict(self, X, y=None):\n","        self.fit(X)\n","        return self.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xzX6w3fdVloG"},"source":["##### Prefix length bucketing\n","\n","Each bucket contains the prefixes of a specific length. For instance, the *nth* bucket contains prefixes where at least *n* events have been performed. **One classifier is built for each possible prefix length**.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Js8nN2i1VpIp","colab":{}},"source":["# Prefix length bucketing. Each bucket contains the prefixes of a specific \n","# length. For instance, the nth bucket contains prefixes where at least n events \n","# have been performed. One classifier is built for each possible prefix length.\n","class PrefixLengthBucketer(object):\n","    \n","    def __init__(self, case_id_col):\n","        self.n_states = 0\n","        self.case_id_col = case_id_col\n","        \n","    def fit(self, X, y=None):\n","        sizes = X.groupby(self.case_id_col).size()\n","        self.n_states = sizes.unique()\n","        return self\n","    \n","    def predict(self, X, y=None):    \n","        return X.groupby(self.case_id_col).size().as_matrix()\n","    \n","    def fit_predict(self, X, y=None):    \n","        self.fit(X)\n","        return self.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2SUcID8FVqF2"},"source":["\n"," ##### Cluster bucketing\n"," \n"," Each bucket represents a cluster that results from applying a clustering algorithmon the encoded prefixes. One classifier is trained for each resulting cluster, considering only the historical prefixes that fall into that particular cluster. At runtime, the cluster of the running case is determined based on its similarity to each of the existing clusters and the corresponding classifier is applied.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0-dpiwtfUd4-","colab":{}},"source":["# Each bucket represents a cluster that results from applying a clustering \n","# algorithmon the encoded prefixes. One classifier is trained for each \n","# resulting cluster, considering only the historical prefixes that fall into that \n","# particular cluster. At runtime, the cluster of the running case is determined \n","# based on its similarity to each of the existing clusters and the corresponding \n","# classifier is applied.\n","class ClusterBasedBucketer(object):\n","    \n","    def __init__(self, encoder, clustering):\n","        self.encoder = encoder\n","        self.clustering = clustering\n","        \n","    def fit(self, X, y=None):\n","        dt_encoded = self.encoder.fit_transform(X)\n","        self.clustering.fit(dt_encoded)\n","        return self\n","    \n","    def predict(self, X, y=None):\n","        dt_encoded = self.encoder.transform(X)\n","        return self.clustering.predict(dt_encoded)\n","    \n","    def fit_predict(self, X, y=None):\n","        self.fit(X)\n","        return self.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6HaCfDq2Vs8H"},"source":["##### State Bucketing\n","\n","It is used in process-aware apporaches where some kind of process representation is derived and a predictor is trained for each state, or dedcision point."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lzxHC1w2VuSV","colab":{}},"source":["# It is used in process-aware apporaches where some kind of process representation \n","# is derived and a predictor is trained for each state, or dedcision point.\n","class StateBasedBucketer(object):\n","    \n","    def __init__(self, encoder):\n","        self.encoder = encoder\n","        self.dt_states = None\n","        self.n_states = 0\n","        \n","    def fit(self, X, y=None):\n","        dt_encoded = self.encoder.fit_transform(X)\n","        self.dt_states = dt_encoded.drop_duplicates()\n","        self.dt_states = self.dt_states.assign(state = range(len(self.dt_states)))\n","        self.n_states = len(self.dt_states)\n","        return self\n","    \n","    def predict(self, X, y=None):\n","        \n","        dt_encoded = self.encoder.transform(X)\n","        dt_transformed = pd.merge(dt_encoded, self.dt_states, how='left')\n","        dt_transformed.fillna(-1, inplace=True)\n","        return dt_transformed[\"state\"].astype(int).as_matrix()\n","    \n","    def fit_predict(self, X, y=None):\n","        self.fit(X)\n","        return self.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9tNKfB7jdlbk"},"source":["#### Prefix Encoding"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GCt67SuooDRc","colab":{}},"source":["def get_encoder(method, case_id_col=None, static_cat_cols=None, static_num_cols=None, dynamic_cat_cols=None, dynamic_num_cols=None, fillna=True, max_events=None):\n","\n","    if method == \"static\":\n","        return StaticTransformer(case_id_col=case_id_col, cat_cols=static_cat_cols, num_cols=static_num_cols, fillna=fillna)\n","\n","    elif method == \"last\":\n","        return LastStateTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, fillna=fillna)\n","\n","    elif method == \"agg\":\n","        return AggregateTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, boolean=False, fillna=fillna)\n","\n","    elif method == \"bool\":\n","        return AggregateTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, boolean=True, fillna=fillna)\n","    \n","    elif method == \"index\":\n","        return IndexBasedTransformer(case_id_col=case_id_col, cat_cols=dynamic_cat_cols, num_cols=dynamic_num_cols, max_events=max_events, fillna=fillna)\n","\n","    else:\n","        print(\"Invalid encoder type\")\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XxQGU9wgh11a"},"source":["##### Static Encoder"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LjQjd017hfyF","colab":{}},"source":["class StaticTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        dt_first = X.groupby(self.case_id_col).first()\n","        \n","        # transform numeric cols\n","        dt_transformed = dt_first[self.num_cols]\n","        \n","        # transform cat cols\n","        if len(self.cat_cols) > 0:\n","            dt_cat = pd.get_dummies(dt_first[self.cat_cols])\n","            dt_transformed = pd.concat([dt_transformed, dt_cat], axis=1)\n","\n","        # fill NA with 0 if requested\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is not None:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        else:\n","            self.columns = dt_transformed.columns\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QlJfBtTPiBgN"},"source":["##### Aggregate Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5VwXHdD6h_KT","colab":{}},"source":["class AggregateTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, boolean=False, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        \n","        self.boolean = boolean\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        # transform numeric cols\n","        if len(self.num_cols) > 0:\n","            dt_numeric = X.groupby(self.case_id_col)[self.num_cols].agg([\"mean\", \"max\", \"min\", \"sum\", \"std\"])\n","            dt_numeric.columns = ['_'.join(col).strip() for col in dt_numeric.columns.values]\n","            \n","        # transform cat cols\n","        print(X)\n","        print(\"#########################\")\n","        dt_transformed = pd.get_dummies(X[self.cat_cols])\n","        print(dt_transformed)\n","        print(\"#########################\")\n","        dt_transformed[self.case_id_col] = X[self.case_id_col]\n","        print(dt_transformed)\n","        print(\"##########################\")\n","        del X\n","        if self.boolean:\n","            dt_transformed = dt_transformed.groupby(self.case_id_col).max()\n","        else:\n","            dt_transformed = dt_transformed.groupby(self.case_id_col).sum()\n","        \n","        print(dt_transformed)\n","        print(\"##########################\")\n","        # concatenate\n","        if len(self.num_cols) > 0:\n","            dt_transformed = pd.concat([dt_transformed, dt_numeric], axis=1)\n","            del dt_numeric\n","        \n","        # fill missing values with 0-s\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is None:\n","            self.columns = dt_transformed.columns\n","        else:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iUT4vrObiMwN"},"source":["##### Index Based Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZcDzwm9SiG_x","colab":{}},"source":["class IndexBasedExtractor(TransformerMixin):\n","    \n","    def __init__(self, cat_cols, num_cols, max_events, fillna=True):\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.max_events = max_events\n","        self.fillna = fillna\n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        # add missing columns if necessary\n","        if self.columns is None:\n","            relevant_num_cols = [\"%s_%s\"%(col, i) for col in self.num_cols for i in range(self.max_events)]\n","            relevant_cat_col_prefixes = tuple([\"%s_%s_\"%(col, i) for col in self.cat_cols for i in range(self.max_events)])\n","            relevant_cols = [col for col in X.columns if col.startswith(relevant_cat_col_prefixes)] + relevant_num_cols\n","            self.columns = relevant_cols\n","        else:\n","            missing_cols = [col for col in self.columns if col not in X.columns]\n","            for col in missing_cols:\n","                X[col] = 0\n","        \n","        self.transform_time = time() - start\n","        return X[self.columns]\n","\n","class IndexBasedTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, max_events=None, fillna=True, create_dummies=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.max_events = max_events\n","        self.fillna = fillna\n","        self.create_dummies = create_dummies\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        grouped = X.groupby(self.case_id_col, as_index=False)\n","        \n","        if self.max_events is None:\n","            self.max_events = grouped.size().max()\n","        \n","        \n","        dt_transformed = pd.DataFrame(grouped.apply(lambda x: x.name), columns=[self.case_id_col])\n","        for i in range(self.max_events):\n","            dt_index = grouped.nth(i)[[self.case_id_col] + self.cat_cols + self.num_cols]\n","            dt_index.columns = [self.case_id_col] + [\"%s_%s\"%(col, i) for col in self.cat_cols] + [\"%s_%s\"%(col, i) for col in self.num_cols]\n","            dt_transformed = pd.merge(dt_transformed, dt_index, on=self.case_id_col, how=\"left\")\n","        dt_transformed.index = dt_transformed[self.case_id_col]\n","        \n","        # one-hot-encode cat cols\n","        if self.create_dummies:\n","            all_cat_cols = [\"%s_%s\"%(col, i) for col in self.cat_cols for i in range(self.max_events)]\n","            dt_transformed = pd.get_dummies(dt_transformed, columns=all_cat_cols).drop(self.case_id_col, axis=1)\n","        \n","        # fill missing values with 0-s\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","\n","        # add missing columns if necessary\n","        if self.columns is None:\n","            self.columns = dt_transformed.columns\n","        else:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","\n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quTyeK1mikmV"},"source":["##### Last State Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Fc1MY1ACieXh","colab":{}},"source":["class LastStateTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","        \n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        dt_last = X.groupby(self.case_id_col).last()\n","        \n","        # transform numeric cols\n","        dt_transformed = dt_last[self.num_cols]\n","        \n","        # transform cat cols\n","        if len(self.cat_cols) > 0:\n","            dt_cat = pd.get_dummies(dt_last[self.cat_cols])\n","            dt_transformed = pd.concat([dt_transformed, dt_cat], axis=1)\n","        \n","        # fill NA with 0 if requested\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is not None:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        else:\n","            self.columns = dt_transformed.columns\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hAwaMGA9jrWU"},"source":["##### Previous State Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jn8ihsqcioy6","colab":{}},"source":["class PreviousStateTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","        \n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        dt_last = X.groupby(self.case_id_col).nth(-2)\n","        \n","        # transform numeric cols\n","        dt_transformed = dt_last[self.num_cols]\n","        \n","        # transform cat cols\n","        if len(self.cat_cols) > 0:\n","            dt_cat = pd.get_dummies(dt_last[self.cat_cols])\n","            dt_transformed = pd.concat([dt_transformed, dt_cat], axis=1)\n","\n","        # add 0 rows where previous value did not exist\n","        dt_transformed = dt_transformed.reindex(X.groupby(self.case_id_col).first().index, fill_value=0)\n","            \n","        # fill NA with 0 if requested\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is not None:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        else:\n","            self.columns = dt_transformed.columns\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3mTx7NPfkcCG"},"source":["#### Preprocessing\n","\n","Adding columns:\n","  - elapsed time\n","  - remaining time"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575080689882,"user_tz":-600,"elapsed":17693,"user":{"displayName":"Catarina Moreira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyK0wF4TYO25RO2JRdFN_0gu6FArdL0ZYCutkO=s64","userId":"07726683220655856721"}},"id":"MIj5xnV5keKJ","outputId":"6541b61e-271e-4651-d7f3-de59ee91482c","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["input_data_folder = MY_WORKSPACE_DIR + \"experiments/main_logs/\"\n","output_data_folder = MY_WORKSPACE_DIR + \"experiments/logdata/\"\n","\n","filenames_bpic2011 = \"bpic2011.csv\"\n","\n","filenames_bpic2012a = \"bpic2012a.csv\"\n","filenames_bpic2012o = \"bpic2012o.csv\"\n","filenames_bpic2012w = \"bpic2012w.csv\"\n","\n","filenames_bpic2015 = \"bpic2015_5.csv\"\n","\n","filenames = [ filenames_bpic2011, filenames_bpic2012a, filenames_bpic2012o, filenames_bpic2012w, filenames_bpic2015 ]\n","timestamp_col = \"Complete Timestamp\"\n","\n","columns_to_remove = [\"label\"]\n","case_id_col = \"Case ID\"\n","\n","def add_remtime_column(group):\n","    group = group.sort_values(timestamp_col, ascending=False)\n","    start_date = group[timestamp_col].iloc[-1]\n","    end_date = group[timestamp_col].iloc[0]\n","\n","    elapsed = group[timestamp_col] - start_date\n","    elapsed = elapsed.fillna(0)\n","    group[\"elapsed\"] = elapsed.apply(lambda x: float(x / np.timedelta64(1, 's')))  # s is for seconds\n","\n","    remtime = end_date - group[timestamp_col]\n","    remtime = remtime.fillna(0)\n","    group[\"remtime\"] = remtime.apply(lambda x: float(x / np.timedelta64(1, 's'))) # s is for seconds\n","\n","    return group\n","\n","for filename in filenames:\n","    print(filename)\n","    #data = pd.read_csv(os.path.join(input_data_folder, filename), sep=\",\")\n","    #data = data.drop([columns_to_remove], axis=1)\n","    #data[timestamp_col] = pd.to_datetime(data[timestamp_col])\n","    #data = data.groupby(case_id_col).apply(add_remtime_column)\n","    #data.to_csv(os.path.join(output_data_folder, filename), sep=\";\", index=False)\n","\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["bpic2011.csv\n","bpic2012a.csv\n","bpic2012o.csv\n","bpic2012w.csv\n","bpic2015_5.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1Ps-ZRgFAKOJ"},"source":["#### Transformers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vbMJo3Ead_GG"},"source":["##### Aggregate Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yR30-vnClQZ7","colab":{}},"source":["class AggregateTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, boolean=False, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        \n","        self.boolean = boolean\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def get_feature_names(self):\n","        return self.columns.tolist()\n","\n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        # transform numeric cols\n","        if len(self.num_cols) > 0:\n","            dt_numeric = X.groupby(self.case_id_col)[self.num_cols].agg([\"mean\", \"max\", \"min\", \"sum\", \"std\"])\n","            dt_numeric.columns = ['_'.join(col).strip() for col in dt_numeric.columns.values]\n","            \n","        # transform cat cols\n","        dt_transformed = pd.get_dummies(X[self.cat_cols])\n","        dt_transformed[self.case_id_col] = X[self.case_id_col]\n","        del X\n","        if self.boolean:\n","            dt_transformed = dt_transformed.groupby(self.case_id_col).max()\n","        else:\n","            dt_transformed = dt_transformed.groupby(self.case_id_col).sum()\n","        \n","        # concatenate\n","        if len(self.num_cols) > 0:\n","            dt_transformed = pd.concat([dt_transformed, dt_numeric], axis=1)\n","            del dt_numeric\n","        \n","        # fill missing values with 0-s\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is None:\n","            self.columns = dt_transformed.columns\n","        else:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lqPkGjeAeD1G"},"source":["##### Index Based Extractor"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"79HR9loWeMkT","colab":{}},"source":["class IndexBasedExtractor(TransformerMixin):\n","    \n","    def __init__(self, cat_cols, num_cols, max_events, fillna=True):\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.max_events = max_events\n","        self.fillna = fillna\n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    def get_feature_names(self):\n","      return self.columns.tolist()\n","      \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        # add missing columns if necessary\n","        if self.columns is None:\n","            relevant_num_cols = [\"%s_%s\"%(col, i) for col in self.num_cols for i in range(self.max_events)]\n","            relevant_cat_col_prefixes = tuple([\"%s_%s_\"%(col, i) for col in self.cat_cols for i in range(self.max_events)])\n","            relevant_cols = [col for col in X.columns if col.startswith(relevant_cat_col_prefixes)] + relevant_num_cols\n","            self.columns = relevant_cols\n","        else:\n","            missing_cols = [col for col in self.columns if col not in X.columns]\n","            for col in missing_cols:\n","                X[col] = 0\n","        \n","        self.transform_time = time() - start\n","        return X[self.columns]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kyPVGv_BePAW"},"source":["##### Index Based Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4i9VUB0KeVeH","colab":{}},"source":["class IndexBasedTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, max_events=None, fillna=True, create_dummies=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.max_events = max_events\n","        self.fillna = fillna\n","        self.create_dummies = create_dummies\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    def get_feature_names(self):\n","      return self.columns.tolist()\n","\n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        grouped = X.groupby(self.case_id_col, as_index=False)\n","        \n","        if self.max_events is None:\n","            self.max_events = grouped.size().max()\n","        \n","        \n","        dt_transformed = pd.DataFrame(grouped.apply(lambda x: x.name), columns=[self.case_id_col])\n","        for i in range(self.max_events):\n","            dt_index = grouped.nth(i)[[self.case_id_col] + self.cat_cols + self.num_cols]\n","            dt_index.columns = [self.case_id_col] + [\"%s_%s\"%(col, i) for col in self.cat_cols] + [\"%s_%s\"%(col, i) for col in self.num_cols]\n","            dt_transformed = pd.merge(dt_transformed, dt_index, on=self.case_id_col, how=\"left\")\n","        dt_transformed.index = dt_transformed[self.case_id_col]\n","        \n","        # one-hot-encode cat cols\n","        if self.create_dummies:\n","            all_cat_cols = [\"%s_%s\"%(col, i) for col in self.cat_cols for i in range(self.max_events)]\n","            dt_transformed = pd.get_dummies(dt_transformed, columns=all_cat_cols).drop(self.case_id_col, axis=1)\n","        \n","        # fill missing values with 0-s\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","\n","        # add missing columns if necessary\n","        if self.columns is None:\n","            self.columns = dt_transformed.columns\n","        else:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","\n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LF7vjVrDeWzK"},"source":["##### Last State Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xlwzv2RkecK-","colab":{}},"source":["class LastStateTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","        \n","    def fit(self, X, y=None):\n","        return self\n","        \n","    def get_feature_names(self):\n","        return self.columns.tolist() \n","\n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        dt_last = X.groupby(self.case_id_col).last()\n","        \n","        # transform numeric cols\n","        dt_transformed = dt_last[self.num_cols]\n","        \n","        # transform cat cols\n","        if len(self.cat_cols) > 0:\n","            dt_cat = pd.get_dummies(dt_last[self.cat_cols])\n","            dt_transformed = pd.concat([dt_transformed, dt_cat], axis=1)\n","        \n","        # fill NA with 0 if requested\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is not None:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        else:\n","            self.columns = dt_transformed.columns\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UZDEIKAzejE7"},"source":["##### Previous State Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d36y8aRahdvS","colab":{}},"source":["class PreviousStateTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","        \n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def get_feature_names(self):\n","        return self.columns.tolist()\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        dt_last = X.groupby(self.case_id_col).nth(-2)\n","        \n","        # transform numeric cols\n","        dt_transformed = dt_last[self.num_cols]\n","        \n","        # transform cat cols\n","        if len(self.cat_cols) > 0:\n","            dt_cat = pd.get_dummies(dt_last[self.cat_cols])\n","            dt_transformed = pd.concat([dt_transformed, dt_cat], axis=1)\n","\n","        # add 0 rows where previous value did not exist\n","        dt_transformed = dt_transformed.reindex(X.groupby(self.case_id_col).first().index, fill_value=0)\n","            \n","        # fill NA with 0 if requested\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is not None:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        else:\n","            self.columns = dt_transformed.columns\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CiF7gYPrhkAe"},"source":["##### Static Transformer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Una706T-hpRa","colab":{}},"source":["class StaticTransformer(TransformerMixin):\n","    \n","    def __init__(self, case_id_col, cat_cols, num_cols, fillna=True):\n","        self.case_id_col = case_id_col\n","        self.cat_cols = cat_cols\n","        self.num_cols = num_cols\n","        self.fillna = fillna\n","        \n","        self.columns = None\n","        \n","        self.fit_time = 0\n","        self.transform_time = 0\n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        start = time()\n","        \n","        dt_first = X.groupby(self.case_id_col).first()\n","        \n","        # transform numeric cols\n","        dt_transformed = dt_first[self.num_cols]\n","        \n","        # transform cat cols\n","        if len(self.cat_cols) > 0:\n","            dt_cat = pd.get_dummies(dt_first[self.cat_cols])\n","            dt_transformed = pd.concat([dt_transformed, dt_cat], axis=1)\n","\n","        # fill NA with 0 if requested\n","        if self.fillna:\n","            dt_transformed = dt_transformed.fillna(0)\n","            \n","        # add missing columns if necessary\n","        if self.columns is not None:\n","            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n","            for col in missing_cols:\n","                dt_transformed[col] = 0\n","            dt_transformed = dt_transformed[self.columns]\n","        else:\n","            self.columns = dt_transformed.columns\n","        \n","        self.transform_time = time() - start\n","        return dt_transformed\n","\n","    def get_feature_names(self):\n","        return self.columns.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"002NkW1kmGao"},"source":["#### Classifiers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fe7If84FmPKd"},"source":["##### Classifier Wrapper"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jfFJ-o1imIyg","colab":{}},"source":["class ClassifierWrapper(object):\n","    \n","    def __init__(self, cls, min_cases_for_training = 30):\n","        self.cls = cls\n","        \n","        self.min_cases_for_training = min_cases_for_training\n","        self.hardcoded_prediction = None\n","\n","    def fit(self, X, y):\n","        # if there are too few training instances, use the mean\n","        if X.shape[0] < self.min_cases_for_training:\n","            self.hardcoded_prediction = np.mean(y)\n","\n","        # if all the training instances are of the same class, use this class as prediction\n","        elif len(set(y)) < 2:\n","            self.hardcoded_prediction = int(y[0])\n","\n","        else:\n","            self.cls.fit(X, y)\n","            return self\n","    \n","    def predict_proba(self, X, y=None):\n","        if self.hardcoded_prediction is not None:\n","            return array([self.hardcoded_prediction] * X.shape[0])\n","                        \n","        else:\n","            #preds_pos_label_idx = np.where(self.cls.classes_ == 0)[0][0]\n","            #preds = self.cls.predict_proba(X)[:,preds_pos_label_idx]\n","            preds = self.cls.predict(X)\n","            return preds\n","        \n","    def fit_predict(self, X, y):\n","        \n","        self.fit(X, y)\n","        return self.predict_proba(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kt4er56GmT7p"},"source":["##### Classifier Factory"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tb4fGZFqmaN4","colab":{}},"source":["def get_classifier(method, n_estimators, max_features=None, learning_rate=None, max_depth=None, random_state=None, subsample=None, colsample_bytree=None, min_cases_for_training=30):\n","\n","    if method == \"rf\":\n","        return ClassifierWrapper(\n","            cls=RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, random_state=random_state),\n","            min_cases_for_training=min_cases_for_training)\n","               \n","    elif method == \"xgb\":\n","        return ClassifierWrapper(\n","            cls=xgb.XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample,\n","                                     max_depth=max_depth, colsample_bytree=colsample_bytree, n_jobs=2),\n","            min_cases_for_training=min_cases_for_training)\n","\n","    else:\n","        print(\"Invalid classifier type\")\n","        return None\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DWS08Wr21zer"},"source":["#### Dataset Manager"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IlEwN9Dk7Pc1"},"source":["##### Dataset Configurations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"12tEdE4U1Ora","colab":{}},"source":["case_id_col = {}\n","activity_col = {}\n","timestamp_col = {}\n","label_col = {}\n","pos_label = {}\n","neg_label = {}\n","dynamic_cat_cols = {}\n","static_cat_cols = {}\n","dynamic_num_cols = {}\n","static_num_cols = {}\n","filename = {}\n","\n","################################################################################\n","#                        BPIC2011 settings                                     #\n","################################################################################\n","dataset = \"bpic2011\"\n","filename[dataset] = \"logdata/bpic2011.csv\"\n","\n","case_id_col[dataset] = \"Case ID\"\n","activity_col[dataset] = \"Activity\"\n","timestamp_col[dataset] = \"Complete Timestamp\"\n","label_col[dataset] = \"remtime\"\n","pos_label[dataset] = \"deviant\"\n","neg_label[dataset] = \"regular\"\n","\n","# features for classifier\n","dynamic_cat_cols[dataset] = [\"Activity\", \"Producer code\", \"Section\", \"Specialism code\", \"group\"]\n","static_cat_cols[dataset] = [\"Diagnosis\", \"Treatment code\", \"Diagnosis code\", \"case Specialism code\", \"Diagnosis Treatment Combination ID\"]\n","dynamic_num_cols[dataset] = [\"Number of executions\", \"duration\", \"month\", \"weekday\", \"hour\"]\n","static_num_cols[dataset] = [\"Age\"]\n","    \n","################################################################################\n","#                        BPIC2015 settings                                     #\n","################################################################################\n","dataset = \"bpic2015_5\"\n","filename[dataset] = \"logdata/bpic2015_5.csv\"\n","\n","case_id_col[dataset] = \"Case ID\"\n","activity_col[dataset] = \"Activity\"\n","timestamp_col[dataset] = \"Complete Timestamp\"\n","label_col[dataset] = \"remtime\"\n","pos_label[dataset] = \"deviant\"\n","neg_label[dataset] = \"regular\"\n","\n","# features for classifier\n","dynamic_cat_cols[dataset] = [\"Activity\", \"monitoringResource\", \"question\", \"Resource\"]\n","static_cat_cols[dataset] = [\"Responsible_actor\"]\n","dynamic_num_cols[dataset] = [\"duration\", \"month\", \"weekday\", \"hour\"]\n","static_num_cols[dataset] = [\"SUMleges\", 'Aanleg (Uitvoeren werk of werkzaamheid)', 'Bouw', 'Brandveilig gebruik (vergunning)', 'Gebiedsbescherming', 'Handelen in strijd met regels RO', 'Inrit/Uitweg', 'Kap', 'Milieu (neutraal wijziging)', 'Milieu (omgevingsvergunning beperkte milieutoets)', 'Milieu (vergunning)', 'Monument', 'Reclame', 'Sloop']\n","static_num_cols[dataset].append('Flora en Fauna')\n","static_num_cols[dataset].append('Brandveilig gebruik (melding)')\n","static_num_cols[dataset].append('Milieu (melding)')\n","\n","################################################################################\n","#                        BPIC2012 A settings                                   #\n","################################################################################\n","dataset = \"bpic2012a\"\n","\n","filename[dataset] = \"logdata/bpic2012a.csv\"\n","\n","case_id_col[dataset] = \"Case ID\"\n","activity_col[dataset] = \"Activity\"\n","timestamp_col[dataset] = \"Complete Timestamp\"\n","label_col[dataset] = \"remtime\"\n","pos_label[dataset] = \"regular\"\n","neg_label[dataset] = \"deviant\"\n","\n","# features for classifier\n","dynamic_cat_cols[dataset] = ['Activity', 'Resource']\n","static_cat_cols[dataset] = []\n","dynamic_num_cols[dataset] = ['open_cases','elapsed']\n","static_num_cols[dataset] = ['AMOUNT_REQ']\n","\n","################################################################################\n","#                        BPIC2012 O settings                                   #\n","################################################################################\n","dataset = \"bpic2012o\"\n","\n","filename[dataset] = \"logdata/bpic2012o.csv\"\n","\n","case_id_col[dataset] = \"Case ID\"\n","activity_col[dataset] = \"Activity\"\n","timestamp_col[dataset] = \"Complete Timestamp\"\n","label_col[dataset] = \"remtime\"\n","pos_label[dataset] = \"regular\"\n","neg_label[dataset] = \"deviant\"\n","\n","# features for classifier\n","dynamic_cat_cols[dataset] = ['Activity', 'Resource']\n","static_cat_cols[dataset] = []\n","dynamic_num_cols[dataset] = ['open_cases','elapsed']\n","static_num_cols[dataset] = ['AMOUNT_REQ']\n","\n","################################################################################\n","#                        BPIC2012 W settings                                   #\n","################################################################################\n","dataset = \"bpic2012w\"\n","\n","filename[dataset] = \"logdata/bpic2012w.csv\"\n","\n","case_id_col[dataset] = \"Case ID\"\n","activity_col[dataset] = \"Activity\"\n","timestamp_col[dataset] = \"Complete Timestamp\"\n","label_col[dataset] = \"remtime\"\n","pos_label[dataset] = \"regular\"\n","neg_label[dataset] = \"deviant\"\n","\n","# features for classifier\n","dynamic_cat_cols[dataset] = ['Activity', 'Resource']\n","static_cat_cols[dataset] = []\n","dynamic_num_cols[dataset] = ['open_cases','elapsed','proctime']\n","static_num_cols[dataset] = ['AMOUNT_REQ']\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zgiOBv1m7KkQ"},"source":["##### Dataset Manager"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"55cCBPpG65Mk","colab":{}},"source":["class DatasetManager:\n","    \n","    def __init__(self, dataset_name):\n","        self.dataset_name = dataset_name\n","        \n","        self.case_id_col = case_id_col[self.dataset_name]\n","        self.activity_col = activity_col[self.dataset_name]\n","        self.timestamp_col = timestamp_col[self.dataset_name]\n","        self.label_col = label_col[self.dataset_name]\n","        self.pos_label = pos_label[self.dataset_name]\n","\n","        self.dynamic_cat_cols = dynamic_cat_cols[self.dataset_name]\n","        self.static_cat_cols = static_cat_cols[self.dataset_name]\n","        self.dynamic_num_cols = dynamic_num_cols[self.dataset_name]\n","        self.static_num_cols = static_num_cols[self.dataset_name]\n","        \n","    def read_dataset(self):\n","        # read dataset\n","        dtypes = {col:\"object\" for col in self.dynamic_cat_cols+self.static_cat_cols+[self.case_id_col, self.timestamp_col]}\n","        for col in self.dynamic_num_cols + self.static_num_cols:\n","            dtypes[col] = \"float\"\n","\n","        dtypes[self.label_col] = \"float\"  # remaining time should be float\n","\n","        data = pd.read_csv( MY_WORKSPACE_DIR + \"experiments/\" + filename[ self.dataset_name], sep=\";\", dtype=dtypes)\n","        data[self.timestamp_col] = pd.to_datetime(data[self.timestamp_col])\n","        return data\n","\n","    def split_data(self, data, train_ratio):  \n","        # split into train and test using temporal split\n","\n","        grouped = data.groupby(self.case_id_col)\n","        start_timestamps = grouped[self.timestamp_col].min().reset_index()\n","        start_timestamps = start_timestamps.sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n","        train_ids = list(start_timestamps[self.case_id_col])[:int(train_ratio*len(start_timestamps))]\n","        train = data[data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n","        test = data[~data[self.case_id_col].isin(train_ids)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n","        return (train, test)\n","\n","    def generate_prefix_data(self, data, min_length, max_length):\n","        # generate prefix data (each possible prefix becomes a trace)\n","        data['case_length'] = data.groupby(self.case_id_col)[self.activity_col].transform(len)\n","\n","        dt_prefixes = data[data['case_length'] >= min_length].groupby(self.case_id_col).head(min_length)\n","        for nr_events in range(min_length+1, max_length+1):\n","            tmp = data[data['case_length'] >= nr_events].groupby(self.case_id_col).head(nr_events)\n","            tmp[self.case_id_col] = tmp[self.case_id_col].apply(lambda x: \"%s_%s\"%(x, nr_events))\n","            dt_prefixes = pd.concat([dt_prefixes, tmp], axis=0)\n","        \n","        dt_prefixes['case_length'] = dt_prefixes.groupby(self.case_id_col)[self.activity_col].transform(len)\n","        return dt_prefixes\n","\n","    def get_pos_case_length_quantile(self, data, quantile=0.90):\n","        return int(np.floor(data.groupby(self.case_id_col).size().quantile(quantile)))\n","\n","    def get_indexes(self, data):\n","        return data.groupby(self.case_id_col).first().index\n","\n","    def get_relevant_data_by_indexes(self, data, indexes):\n","        return data[data[self.case_id_col].isin(indexes)]\n","\n","    def get_label(self, data):\n","        return data.groupby(self.case_id_col).min()[self.label_col]\n","    \n","    def get_label_numeric(self, data):\n","        y = self.get_label(data) # one row per case\n","        #return [1 if label == self.pos_label else 0 for label in y]\n","        return y\n","    \n","    def get_class_ratio(self, data):\n","        class_freqs = data[self.label_col].value_counts()\n","        return class_freqs[self.pos_label] / class_freqs.sum()\n","    \n","    def get_stratified_split_generator(self, data, n_splits=5, shuffle=True, random_state=22):\n","        grouped_firsts = data.groupby(self.case_id_col, as_index=False).first()\n","        skf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n","        \n","        for train_index, test_index in skf.split(grouped_firsts, grouped_firsts[self.label_col]):\n","            current_train_names = grouped_firsts[self.case_id_col][train_index]\n","            train_chunk = data[data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n","            test_chunk = data[~data[self.case_id_col].isin(current_train_names)].sort_values(self.timestamp_col, ascending=True, kind='mergesort')\n","            yield (train_chunk, test_chunk)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UvMXPAJ6a31I"},"source":["Adding LIME utility function\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YOn_G3n8mSCE","colab":{}},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def imp_df(column_names, importances):\n","        df = pd.DataFrame({'feature': column_names,\n","                       'feature_importance': importances}) \\\n","           .sort_values('feature_importance', ascending = False) \\\n","           .reset_index(drop = True)\n","        return df\n","\n","# plotting a feature importance dataframe (horizontal barchart)\n","def var_imp_plot(imp_df, title, num_feat):\n","        imp_df.columns = ['feature', 'feature_importance']\n","        #plt.figure( figsize=(25,10))\n","        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\") \n","        b.set_title(title, fontsize = 14) \n","    \n","        for item in b.get_yticklabels():\n","            item.set_fontsize(13)\n","\n","        #%get_backend()\n","        fig2 = b.get_figure()\n","        \n","        print(\"\\nSAVING FIGURE\\n\")\n","\n","        print(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + bucket_encoding + \"/lime/chunk\" + str(part) + \"/FeatureImportance_\" + dataset_ref + \"_G\" + str(gen_counter)+ \"_p\" + str(part) + \"_e\" + str(nr_events)+ \"_b\" + str(bucket) +\"_\" + str(bucketer_params_combo) + \"_\" + str(cls_params_combo)+ \".png\")\n","        fig2.show()\n","        fig2.savefig(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + bucket_encoding + \"/lime/chunk\" + str(part) + \"/FeatureImportance_\" + dataset_ref + \"_G\" + str(gen_counter)+\"_p\" + str(part) + \"_e\" + str(nr_events)+ \"_b\" + str(bucket) +\"_\" + str(bucketer_params_combo) + \"_\" + str(cls_params_combo)+ \".png\",\n","                    bbox_inches='tight',dpi=300)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZCPCwrMya_Aj","colab":{}},"source":["\n","def generate_local_explanations(explainer,test_xi, cls,test_y, num_vars = 6):\n","    \n","    print(\"Actual value \", test_y)\n","    num_features=6;# maximum is 6 ,if it is larger than 6, the features displayed are different.\n","\n","    exp = None\n","    try:\n","      exp = explainer.explain_instance(test_xi, cls.predict_proba, num_features=num_features)\n","      exp.show_in_notebook(show_table=True, show_all=False)\n","      print ('Explanation for class %s' )\n","      print ('\\n'.join(map(str, exp.as_list())))\n","\n","    except ValueError:\n","      print(\"#################################\")\n","      print(\"EXCEPTION\")\n","      print(\"#################################\")\n","\n","    #probability_result=cls.predict_proba([test_xi])[0];\n","    #print(probability_result);\n","    \n","    return exp"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gVWc4GIPpSxO"},"source":["#### Train Classifiers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FzpphkjXpizL"},"source":["##### Train Classifier XgBoost"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ir-4-x83oGsG","colab":{}},"source":["# trying to get the datset out of the pipeline process\n","from sklearn.base import TransformerMixin, BaseEstimator\n","\n","class Debug(BaseEstimator, TransformerMixin):\n","\n","    def transform(self, X):\n","        #self.shape = shape\n","\n","        # what other output you want\n","        return X\n","\n","    def fit(self, X, y=None, **fit_params):\n","\n","        X_train = pd.DataFrame( X )\n","        y_train = pd.DataFrame( y )\n","\n","        print(\"[DEBUG] Writing file: \" + MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/train/chunk\" + str(part) + \"/Xtrain_\" + dataset_ref + \"_p\" + str(part) + \"_b\" + str(bucket) + \".csv\")\n","        X_train.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/train/chunk\" + str(part) + \"/Xtrain_\" + dataset_ref + \"_p\" + str(part) + \"_b\" + str(bucket) + \".csv\", index=False)\n","        y_train.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/train/chunk\"  + str(part) + \"/ytrain_\" + dataset_ref  + \"_p\" + str(part) + \"_b\" + str(bucket) + \".csv\", index=False)\n","        \n","        return self\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1575081537687,"user_tz":-600,"elapsed":865449,"user":{"displayName":"Catarina Moreira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyK0wF4TYO25RO2JRdFN_0gu6FArdL0ZYCutkO=s64","userId":"07726683220655856721"}},"id":"0PBXcN2ZpPBQ","outputId":"7f283045-1bf5-407e-801e-f1fedd3b2b45","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1D1sAuM8vj5fnhAOjkLMYK3FydubWiVoS"}},"source":["import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","# -----------------------------------------------------------------------\n","# TODO: in BPIC2011 use only the \"other\" treatment code and check results\n","# --------------------------------------------------------------\n","\n","# SIngle bucket, aggregation encoding\n","# bucket_method = single\n","# cls_encoding = agg\n","\n","# prefix bucket, aggregation encoding\n","# bucket_method = prefix\n","# cls_encoding = agg\n","\n","# prefix bucket, index encoding\n","# bucket_method = prefix\n","# cls_encoding = index\n","\n","# Run LSTM\n","dataset_ref = \"bpic2011\"\n","\n","bucket_method = \"single\"\n","bucket_encoding = \"agg\"\n","\n","cls_encoding = \"agg\"\n","cls_method = \"xgb\"\n","\n","results_dir = MY_WORKSPACE_DIR + \"results/\"\n","\n","if bucket_method == \"state\":\n","    bucket_encoding = \"last\"\n","\n","method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n","\n","home_dir = MY_WORKSPACE_DIR\n","\n","if not os.path.exists(os.path.join(home_dir, results_dir)):\n","    os.makedirs(os.path.join(home_dir, results_dir))\n","\n","dataset_ref_to_datasets = {\n","    \"bpic2011\": [\"bpic2011\"],\n","    \"bpic2012a\": [\"bpic2012a\"],\n","    \"bpic2012o\": [\"bpic2012o\"],\n","    \"bpic2012w\": [\"bpic2012w\"],\n","    \"bpic2015\": [\"bpic2015_5\"],\n","    \"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n","    \"bpic2017\": [\"bpic2017\"]\n","}\n","\n","encoding_dict = {\n","    \"laststate\": [\"static\", \"last\"],\n","    \"agg\": [\"static\", \"agg\"],\n","    \"index\": [\"static\", \"index\"],\n","    \"combined\": [\"static\", \"last\", \"agg\"]}\n","    \n","datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n","methods = encoding_dict[cls_encoding]\n","\n","# bucketing params to optimize \n","if bucket_method == \"cluster\":\n","    bucketer_params = {'n_clusters':[2, 4, 6]}\n","else:\n","    bucketer_params = {'n_clusters':[1]}\n","\n","# classification params to optimize\n","if cls_method == \"rf\":\n","    cls_params = {'n_estimators':[250, 500],\n","                  'max_features':[\"sqrt\", 0.1, 0.5, 0.75]}\n","    \n","elif cls_method == \"xgb\":\n","    cls_params = {'n_estimators':[500],\n","                  'learning_rate':[0.06],\n","                  'subsample':[0.8],\n","                  'max_depth': [3, 5, 7],\n","                  'colsample_bytree': [0.6, 0.9]}\n","\n","bucketer_params_names = list(bucketer_params.keys())\n","cls_params_names = list(cls_params.keys())\n","\n","outfile = os.path.join(home_dir, results_dir, \"cv_results_%s_%s_%s.csv\"%(cls_method, method_name, dataset_ref))\n","print(outfile)\n","\n","train_ratio = 0.8\n","random_state = 22\n","fillna = True\n","n_min_cases_in_bucket = 30\n","    \n","##### MAIN PART ######    \n","with open(outfile, 'w') as fout:\n","    \n","    fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(\"part\", \"dataset\", \"method\", \"cls\", \";\".join(bucketer_params_names), \";\".join(cls_params_names), \"nr_events\", \"metric\", \"score\"))\n","    \n","    gen_counter = 0\n","    nr_events = 0\n","    exp_indx = 0\n","    for dataset_name in datasets:\n","        \n","        dataset_manager = DatasetManager( dataset_name )\n","        \n","        # read the data\n","        data = dataset_manager.read_dataset()\n","        \n","        # split data into train and test\n","        train, _ = dataset_manager.split_data(data, train_ratio)\n","        \n","        # consider prefix lengths until 90% of positive cases have finished\n","        min_prefix_length = 1\n","        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n","        del data\n","        \n","        part = 0\n","        for train_chunk, test_chunk in dataset_manager.get_stratified_split_generator(train, n_splits=3):\n","            part += 1\n","            print(\"Starting chunk %s...\"%part)\n","            sys.stdout.flush()\n","            \n","            # create prefix logs\n","            dt_train_prefixes = dataset_manager.generate_prefix_data(train_chunk, min_prefix_length, max_prefix_length)\n","            dt_test_prefixes = dataset_manager.generate_prefix_data(test_chunk, min_prefix_length, max_prefix_length)\n","            \n","            print(dt_train_prefixes.shape)\n","            print(dt_test_prefixes.shape)\n","\n","            #creating a dictionary to store explanations ####ADDED BY RENUKA\n","            exp_dict=dict()\n","\n","            # #####################################################################\n","            # GET DATASET BY CHUNKS\n","\n","            df_train = pd.DataFrame( dt_train_prefixes )\n","            df_train.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding +\"/train/chunk\"  + str(part) + \"/train_\" + dataset_ref  + \"_p\" + str(part) + \".csv\", index=False)\n","            \n","            df_test = pd.DataFrame( dt_test_prefixes )\n","            df_test.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/test/chunk\"  + str(part) + \"/test_\" + dataset_ref  + \"_p\" + str(part) + \".csv\", index=False)\n","\n","            # #####################################################################\n","\n","            for bucketer_params_combo in itertools.product(*(bucketer_params.values())):\n","                for cls_params_combo in itertools.product(*(cls_params.values())):\n","                    print(\"Bucketer params are: %s\"%str(bucketer_params_combo))\n","                    print(\"Cls params are: %s\"%str(cls_params_combo))\n","\n","                    # extract arguments\n","                    bucketer_args = {'encoding_method':bucket_encoding, \n","                                     'case_id_col':dataset_manager.case_id_col, \n","                                     'cat_cols':[dataset_manager.activity_col], \n","                                     'num_cols':[], \n","                                     'random_state':random_state}\n","\n","                    for i in range(len(bucketer_params_names)):\n","                        bucketer_args[bucketer_params_names[i]] = bucketer_params_combo[i]\n","\n","                    cls_encoder_args = {'case_id_col':dataset_manager.case_id_col, \n","                                        'static_cat_cols':dataset_manager.static_cat_cols,\n","                                        'static_num_cols':dataset_manager.static_num_cols, \n","                                        'dynamic_cat_cols':dataset_manager.dynamic_cat_cols,\n","                                        'dynamic_num_cols':dataset_manager.dynamic_num_cols, \n","                                        'fillna':fillna}\n","\n","                    print( cls_encoder_args )\n","\n","                    cls_args = {'random_state':random_state,\n","                                'min_cases_for_training':n_min_cases_in_bucket}\n","\n","                    for i in range(len(cls_params_names)):\n","                        cls_args[cls_params_names[i]] = cls_params_combo[i]\n","        \n","                    # Bucketing prefixes based on control flow\n","                    print(\"Bucketing prefixes...\")\n","                    bucketer = get_bucketer(bucket_method, **bucketer_args)\n","                    bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n","\n","                    pipelines = {}\n","                    explainers = {}\n","\n","                    # train and fit pipeline for each bucket\n","                    count = 0 # storing a few explanations - not all\n","                    for bucket in set(bucket_assignments_train):\n","\n","                        print(\"Fitting pipeline for bucket %s...\"%bucket)\n","                        relevant_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucket]\n","                        dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_cases_bucket) # one row per event\n","                        train_y = dataset_manager.get_label_numeric(dt_train_bucket)\n","\n","                        feature_combiner = FeatureUnion([(method, get_encoder(method, **cls_encoder_args)) for method in methods])\n","                        pipelines[bucket] = Pipeline([('encoder', feature_combiner), ('debug', Debug()), ('cls', get_classifier(cls_method, **cls_args))])\n","                        pipelines[bucket].fit(dt_train_bucket, train_y)\n","\n","                        # GENERAL EXPLANATIONS\n","                        #sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n","                        #sns.set\n","\n","                        print( \"Generating general explanations...\" )\n","                        \n","                        feat_names = feature_combiner.get_feature_names()\n","                        base_imp = imp_df(feat_names, pipelines[bucket].named_steps['cls'].cls.feature_importances_ )\n","                        base_imp.head(6)\n","                        var_imp_plot(base_imp, 'Feature importance using Random forest', 6)\n","                        \n","                        # SERIALIZE MODEL\n","                        pickle.dump(pipelines[bucket], open(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/train/chunk\" + str(part) + \"/model_\" + dataset_ref + \"_p\" + str(part) + \"_b\" + str(bucket) +  \"_\" + str(bucketer_params_combo) + \"_\" + str(cls_params_combo) + \".dat\", \"wb\"))\n","\n","                        #####ADDED BY RENUKA - \n","\n","                        #get the training data as a matrix\n","                        trainingdata = feature_combiner.fit_transform(dt_train_bucket)\n","\n","                        #Did not use categorical features as the parameter - example code of lime says use it,check this out.\n","                        explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata, feature_names=feature_combiner.get_feature_names(), class_names=['remtime'], verbose=True, mode='regression')\n","                        print(explainer)\n","\n","                        # write down feature names\n","\n","                        feat = pd.DataFrame( feature_combiner.get_feature_names() )\n","                        feat.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/train/chunk\" + str(part) + \"/Xtrain_Features_\" + dataset_ref + \"_p\" + str(part) + \"_b\" + str(bucket) + \".csv\", index=False)\n","                        \n","                     \n","                        # if the bucketing is prefix-length-based, then evaluate for each prefix length separately, otherwise evaluate all prefixes together \n","                        max_evaluation_prefix_length = max_prefix_length if bucket_method == \"prefix\" else min_prefix_length\n","                        \n","                        prefix_lengths_test = dt_test_prefixes.groupby(dataset_manager.case_id_col).size()\n","\n","                        print(max_evaluation_prefix_length)\n","                        for nr_events in range(min_prefix_length, max_evaluation_prefix_length+1):\n","                            gen_counter = gen_counter + 1\n","                            print(\"Predicting for %s events...\"%nr_events)\n","\n","                            if bucket_method == \"prefix\":\n","\n","                                # select only prefixes that are of length nr_events\n","                                relevant_cases_nr_events = prefix_lengths_test[prefix_lengths_test == nr_events].index\n","\n","                                if len(relevant_cases_nr_events) == 0:\n","                                    break\n","\n","                                dt_test_nr_events = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_cases_nr_events)\n","                                del relevant_cases_nr_events\n","                            else:\n","                                # evaluate on all prefixes\n","                                dt_test_nr_events = dt_test_prefixes.copy()\n","\n","                            start = time()\n","                            # get predicted cluster for each test case\n","                            bucket_assignments_test = bucketer.predict(dt_test_nr_events)\n","\n","                            #### WRITE DOWN TEST RESULTS\n","\n","                            X_test = pd.DataFrame( dt_test_nr_events )\n","                            X_test.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/test/X_test_\" + dataset_ref  + \"_p\" + str(part) + \"_e\"+str(nr_events) + \".csv\")\n","            \n","                            ################################\n","\n","                            # use appropriate classifier for each bucket of test cases\n","                            # for evaluation, collect predictions from different buckets together\n","                            preds = []\n","                            test_y = []\n","\n","                            relevant_cases_bucket = dataset_manager.get_indexes(dt_test_nr_events)[bucket_assignments_test == bucket]\n","                            dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_nr_events, relevant_cases_bucket) # one row per event\n","\n","                            if len(relevant_cases_bucket) == 0:\n","                                continue\n","\n","                            elif bucket not in pipelines:\n","                                # use mean remaining time (in training set) as prediction\n","                                preds_bucket = array([np.mean(train_chunk[\"remtime\"])] * len(relevant_cases_bucket))\n","                                # preds_bucket = [dataset_manager.get_class_ratio(train_chunk)] * len(relevant_cases_bucket)\n","\n","                            else:\n","                                # make actual predictions\n","                                preds_bucket = pipelines[bucket].predict_proba(dt_test_bucket)\n","                                \n","                                ####ADDED BY RENUKA - get the explanation\n","                                test_y_bucket = dataset_manager.get_label_numeric(dt_test_bucket)\n","                                test_x =  feature_combiner.fit_transform(dt_test_bucket)[0]\n","\n","                                print(\"Getting classifier...\")\n","                                #cls = pickle.load(open(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + bucket_encoding + \"/train/chunk\" + str(part) + \"/model_\" + dataset_ref + \"_p\" + str(part) + \"_b\" + str(bucket) +  \"_\" + str(bucketer_params_combo) + \"_\" + str(cls_params_combo) + \".dat\", \"rb\"))\n","                                cls = pipelines[bucket].named_steps['cls']\n","                                exp=generate_local_explanations(explainer, test_x, cls, test_y_bucket, feature_combiner)\n","                                \n","                                print('Generating local Explanations')\n","                                \n","                                exp_dict[exp_indx]=exp\n","                                exp_indx = exp_indx+1\n","                                \n","                                #rc={'axes.labelsize': 12, 'xtick.labelsize': 13, 'ytick.labelsize': 13 , 'axes.titlesize': 10}\n","                                #sns.set(rc)\n","                                #sns.set_style(\"whitegrid\")\n","                                #%matplotlib inline\n","                                \n","                                print('Explanations for prefix length ', bucket)\n","                                \n","                                #fig = exp.as_pyplot_figure()\n","                                \n","                                #print(\"\\nSAVING FIGURE...\\n\")\n","                                #gen_counter = gen_counter+1\n","                                #print(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/lime/chunk\" + str(part) + \"/Local_Expl_\" + dataset_ref + \"_p\" + str(part) + \"_e\" + str(nr_events) + \"_b\" + str(bucket) +\"_\" + str(bucketer_params_combo) + \"_\" + str(cls_params_combo) + \".png\")\n","                            \n","                                #fig.savefig(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/lime/chunk\" + str(part) + \"/Local_Expl_\" + dataset_ref+ \"_G\" + str(gen_counter) + \"_p\" + str(part) + \"_e\" + str(nr_events) + \"_b\" + str(bucket) + \".png\",\n","                                #           bbox_inches='tight',dpi=300)\n","                                #gen_counter = gen_counter+1\n","\n","                                \n","                                print('Explanations for prefix length ', bucket)\n","                      \n","                                count=count+1\n","                                ##############\n","                            \n","\n","                        preds_bucket = preds_bucket.clip(min=0)  # if remaining time is predicted to be negative, make it zero\n","                        preds.extend(preds_bucket)\n","\n","                        # extract actual label values\n","                        test_y_bucket = dataset_manager.get_label_numeric(dt_test_bucket) # one row per case\n","                        test_y.extend(test_y_bucket)\n","\n","\n","                        ##### WRITE DOWN RESUTLS\n","                        y_test = pd.DataFrame( test_y )\n","                        y_test.to_csv( MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/test/y_test_\" + dataset_ref  + \"_p\" + str(part) + \".csv\")\n","                        ##########################\n","\n","                    if len(test_y) < 2:\n","                        mae = None\n","                    else:\n","                        mae = mean_absolute_error(test_y, preds)\n","                      \n","                    #prec, rec, fscore, _ = precision_recall_fscore_support(test_y, [0 if pred < 0.5 else 1 for pred in preds], average=\"binary\")\n","                    bucketer_params_str = \";\".join([str(param) for param in bucketer_params_combo])\n","                    cls_params_str = \";\".join([str(param) for param in cls_params_combo])\n","\n","                    print([part, dataset_name, method_name, cls_method, bucketer_params_str, cls_params_str, nr_events, \"mae\", mae])\n","                    fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(part, dataset_name, method_name, cls_method, bucketer_params_str, cls_params_str, nr_events, \"mae\", mae))\n","                    #fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(part, dataset_name, method_name, cls_method, bucketer_params_str, cls_params_str, nr_events, \"precision\", prec))\n","                    #fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(part, dataset_name, method_name, cls_method, bucketer_params_str, cls_params_str, nr_events, \"recall\", rec))\n","                    #fout.write(\"%s;%s;%s;%s;%s;%s;%s;%s;%s\\n\"%(part, dataset_name, method_name, cls_method, bucketer_params_str, cls_params_str, nr_events, \"fscore\", fscore))\n","\n","                print(\"\\n\")\n"],"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Hv_lBNMJiL_Z"},"source":["LIME Explanations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8u2riTFQiPD_","colab":{}},"source":["rc={'axes.labelsize': 12, 'xtick.labelsize': 13, 'ytick.labelsize': 13 , 'axes.titlesize': 10}\n","sns.set(rc)\n","sns.set_style(\"whitegrid\")\n","%matplotlib inline\n","for pre, exp in exp_dict.items():\n","    print('Explanations for prefix length ', bucket)\n","    fig = exp.as_pyplot_figure()\n","    fig.show()\n","    fig.savefig(MY_WORKSPACE_DIR + \"XGBoost/buckets_\" + bucket_method + \"_\" + cls_encoding + \"/lime/chunk\" + str(part) + \"/Local_Expl_\" + dataset_ref + \"_p\" + str(pre) + \"_b\" + str(bucket) + \".png\",bbox_inches='tight',dpi=300)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbr5KnCOg00c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}